{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESS: Analysing and interpreting FDEM data - PC class 2\n",
    "---\n",
    "# 0 - Introduction\n",
    "To evaluate to which extent the collected FDEM data can help predict the spatial variability of your group's target properties, it is important to understand the relationship between the sensor output, and the targeted properties. In this notebook, you will be able to explore the underlying pedophysical relationships between the bulk electrical conductivity of a soil sample, and specific soil properties.\n",
    "\n",
    "In a final segment of the notebook, you can develop stochastic models that can help predict the target property you have to explore.\n",
    "\n",
    "In the first code cells, you once again import the required modules, as well as some basic functions that were deployed in the previous notebooks that you can use here as well.\n",
    "In **_code cell 0.2_** the different available datasets are imported. Here you can also import your own datasets (such as inversion results etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for setup\n",
    "# -------------------------------------------- #\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the repository path:\n",
    "# If the code is run in Google Colab This code will check if the repository is already cloned, if not it will clone it\n",
    "# If the code is run locally, the repository path is set to the local path\n",
    "# -------------------------------------------- #\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    repo_path = r'\\content\\I002658A_ESS'\n",
    "    if not os.path.exists(repo_path):\n",
    "        !git clone https://github.com/SENSE-UGent/I002658A_ESS.git\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path)\n",
    "\n",
    "else:\n",
    "\n",
    "    repo_path = r'c:\\Users\\pdweerdt\\Documents\\Repos\\I002658A_ESS' # Change this to the location of the repository on your machine\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path) \n",
    "\n",
    "# Import the setup function\n",
    "from Utils.setup import check_and_install_packages\n",
    "\n",
    "# Read the requirements.txt file\n",
    "\n",
    "requirements_path = repo_path + r'\\Utils\\requirements.txt'\n",
    "\n",
    "with open(requirements_path) as f:\n",
    "    required_packages = f.read().splitlines()\n",
    "\n",
    "# Check and install packages\n",
    "check_and_install_packages(required_packages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the utility functions\n",
    "from Utils.func import (\n",
    "                        interpolate, export_grid, lin_sens, \n",
    "                        update_plot, update_plot2, \n",
    "                        waxsmits, linde, fu\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0.0 Import the required modules to run all code in this notebook.\n",
    "    There is redundancy in the imports, but this is to ensure that all code\n",
    "    can be run without having to worry about missing modules. Even if you \n",
    "    combine the notebook with cells from the first practicum.\n",
    "'''\n",
    "# General utility modules\n",
    "import warnings\n",
    "import re\n",
    "import requests\n",
    "\n",
    "# Modules for geopunt data visualisation\n",
    "from IPython.display import HTML\n",
    "from IPython.core.display import display\n",
    "from ipywidgets import widgets, HBox, interact, FloatSlider\n",
    "\n",
    "# Data visualisation, manipulation, and analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from scipy.spatial import cKDTree\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.optimize import root\n",
    "\n",
    "# Geospatial data manipulation and raster operations\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.transform import from_origin\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from scipy.ndimage import gaussian_filter\n",
    "\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# EMI 1D inversion package (emagpy)\n",
    "from emagpy import Problem\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Working Directory\n",
    "\n",
    "Do this to simplify subsequent reads and writes (avoid including the full address each time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For use in Google Colab\n",
    "\n",
    "Run the following cell if you automatically want to get the data from the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print('Current Working Directory is ', cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For local use\n",
    "\n",
    "Only run the following cell if you have the data locally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory, place an r in front to address special characters\n",
    "os.chdir(r'c:\\Users\\pdweerdt\\Documents\\Repos')\n",
    "\n",
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print('Current Working Directory is ', cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Export directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting dataframes, figures can also be exported to a specific folder. These files can be opened later for further analysis. It is important to know where you store your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  let's have a look again at our current working directory\n",
    "cd = os.getcwd()\n",
    "print('Current Working Directory is ', cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For saving files locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define the path where to export results, make sure the folder exists (create it if necessary)\n",
    "export_path = cd + r'\\I002658A_ESS\\Results'\n",
    "\n",
    "print('Results will be exported to: ', export_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For saving files on Google Drive\n",
    "\n",
    "In Google Colab, the contents folder files are temporarily. When the runtime is termminated, all data is lost. However, if you mount your Google Drive first, you can read and write files directly from and onto your Google Drive folders. In this way, your data is stored on the cloud and you can access it anywhere with your Google account.\n",
    "\n",
    "Mount Drive first to Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    from google.colab import drive # import the drive method for mounting Google Drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "else:\n",
    "    \n",
    "    print('Google Colab is not running, not mounting Google Drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will probably get a message to authorize mounting Drive on Colab.\n",
    "\n",
    "Next up we can set the filte path in Google Drive, below is the general code for saving files on your Drive Parent folder, best is to save it into a subfolder for this course for example: <br> **'/content/drive/My Drive/EnvSoilSens/Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path in Google Drive\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    export_path = '/content/drive/My Drive'\n",
    "\n",
    "    print(\"Results will be exported to: \", export_path)\n",
    "\n",
    "else:\n",
    "    \n",
    "    print('Google Colab is not running, not mounting Google Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0.2 Basic functions\n",
    "'''\n",
    "\n",
    "# 0.2a Functions for interpolating and exporting data\n",
    "# ----------------------------------------------------\n",
    "\n",
    "help(interpolate)\n",
    "help(export_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Field dataset - Testfield Proefhoeve Bottelaere [Vijverhoek, Oosterzele]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "0.1 Import datasets\n",
    "*******************\n",
    "\n",
    "Here you can import the datasets that you will use in the notebook. The datasets\n",
    "consist of field data collected during the ESS2025 field campaign, which are \n",
    "provided via the URLs below. \n",
    "You can also import datasets that you have generated in previous notebooks. \n",
    "For those you can indicate the location on google drive (with Google Colab), or\n",
    "on your local machine (with Jupyter Notebook/your own IDE).\n",
    "\n",
    "'''\n",
    "# Location of datasets generated in PC class 1 (02_ESS-... .ipynb)\n",
    "FDEM_inverted_all = [] #dataset location Google Drive or local machine (string)\n",
    "FDEM_inverted_samples = [] #dataset location Google Drive or local machine (string)\n",
    "\n",
    "# Location of field datasets\n",
    "EC_logs = cd + r'\\I002658A_ESS\\Data\\Survey2025\\EC_logs_2024.csv'\n",
    "samples = cd + r'\\I002658A_ESS\\Data\\Survey2025\\samples_2024.csv' \n",
    "\n",
    "# **************************************************************************** #\n",
    "\n",
    "# Provided dataset locations\n",
    "FDEM_surveydata = cd + r'\\I002658A_ESS\\Data\\Survey2025\\FDEM_2024.csv'\n",
    "FDEM_transect = cd + r'\\I002658A_ESS\\Data\\Survey2025\\FDEM_transect_2024.csv'\n",
    "\n",
    "\n",
    "if FDEM_inverted_samples == []:\n",
    "    FDEM_inverted_samples = cd + r'\\I002658A_ESS\\Data\\Example\\FDEM_inv_samples_example.csv'\n",
    "    warnings.warn('No dataset provided for FDEM_inverted_samples. Using example dataset instead.')\n",
    "if FDEM_inverted_all == []:\n",
    "    FDEM_inverted_all = cd + r'\\I002658A_ESS\\Data\\Example\\FDEM_inverted_all_example.csv'\n",
    "    warnings.warn('No dataset provided for FDEM_inverted_all. Using example dataset instead.')\n",
    "\n",
    "# URL for grid masking file\n",
    "blank_json = cd + r'\\I002658A_ESS\\Data\\Survey2025\\blank.json'\n",
    "\n",
    "# Create dataframes from datasets\n",
    "'''\n",
    "Import datasets as dataframes\n",
    "-----------------------------\n",
    "    - df = dataframe with the full FDEM dataset\n",
    "    - dt = dataframe with the FDEM transect\n",
    "    - ds = datasframe with the sample data (including analytical data)\n",
    "    - blank = geojson (polygon) outlining survey extent\n",
    "'''\n",
    "df = pd.read_csv(FDEM_surveydata, sep=',', header=0)\n",
    "dt = pd.read_csv(FDEM_transect, sep=',', header=0)\n",
    "ds = pd.read_csv(samples, sep=',', header=0)\n",
    "d_inv = pd.read_csv(FDEM_inverted_all, sep=',', header=0)\n",
    "d_inv_samps = pd.read_csv(FDEM_inverted_samples, sep=',', header=0)\n",
    "d_ec_logs = pd.read_csv(EC_logs, sep=',', header=0)\n",
    "blank_in = gpd.read_file(blank_json)\n",
    "blank = blank_in.to_crs('EPSG:31370')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Comparing FDEM data to analytical information at sampling locations\n",
    "In **_code cell 1.0_** you can plot data of a specific coil pair, along with its cumulative sensitivities. \n",
    "In addition, in the third plot you can compare sample data (such as clay content, bulk density, ...) to ECa data collected at those locations with the different coil pairs. This way, you can already visually inspect the correlation between the recorded ECa values and the sampled soil properties.\n",
    "\n",
    "In **_code cell 1.1_** you can compare the outcomes of your inversion at the sampling locations to the data from the EC logs.\n",
    "\n",
    "The columns containing soil sample data in the ds (and d_inv_samps) dataframe and their units are presented in Table 1. Columns are only shown for samples collected at 10 cm depth, as units and datatypes are similar for 50 cm depth data.\n",
    "> \n",
    ">|Column name|unit|datatype|\n",
    ">|-----------|--------|--------|\n",
    ">| *pH_H2O_10cm* | [-]|  pH water |\n",
    ">| *bd_10cm* | [g/cm^3]|bulk density |\n",
    ">| *vwc_10cm* | [%]| volumetric water content | \n",
    ">| *por_10cm* | [-]| porosity |\n",
    ">| *CEC_10cm* | [meq/100g]| cation exchange capacity |\n",
    ">| *clay_10cm* | [%]| clay content |\n",
    ">| *silt_10cm* | [%]| silt content |\n",
    ">| *sand_10cm* | [%]| sand content |\n",
    ">| *TOC_10cm* | [g/kg]| total organic carbon |\n",
    ">| *vmc_hydra* | [%]|  HydraProbe volumetric water content |\n",
    ">| *EC_bulk_hydra* |[mS/m]|  HydraProbe bulk soil electrical conductivity |\n",
    ">| *EC_water_hydra* | [mS/m]| HydraProbe soil water electrical conductivity |\n",
    ">\n",
    "> *Table 1: overview of sample data types.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.0: Evaluating FDEM data along the reference transect\n",
    "# ------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Plotting the relative sensitivity of the QP (LIN ECa) response of the \n",
    "deployed coil configurations. \n",
    "------------------------------------------------------------------------\n",
    "For the third plot, you can evaluate the analytical data at the sampling \n",
    "locations (sample_col), and compare these to a selected FDEM dataset (fdem_col)\n",
    "You can select the desired variables in the two lines of code below this \n",
    "comment. You can change these strings to visually evaluate the relationships\n",
    "# between the analysed properties and the output from different coil pairs.\n",
    "\"\"\"\n",
    "\n",
    "save_to_file = False # set to True if you want to save the plots to a PDF\n",
    "\n",
    "sample_col = 'clay_10cm' # change into, e.g., 'clay_10cm [%]' or 'TOC_10cm  [g/cm3]'\n",
    "sample_col_unit = '[%]' # change into, e.g., 'clay_10cm [%]' or 'TOC_10cm  [g/cm3]'\n",
    "fdem_col = 'HCP0.5' # change into, for instance, 'PRP1.0' or 'HCP2.0_inph'\n",
    "\n",
    "# The following code assumes that the fdem_col is a string that matches one of\n",
    "# the datasets in the full FDEM survey dataset. If you are comparing other \n",
    "# datasets the plotted survey dataset will default to the 1.0 m HCP dataset \n",
    "# unless you change the fdem_plot variable to match the dataset you want to \n",
    "# compare to.\n",
    "fdem_plot = fdem_col\n",
    "\n",
    "maximum_depth = 2 # maximum depth to which you want to invert the data\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize=(21,7))\n",
    "ax1 = axes[0]\n",
    "ax2 = axes[1]\n",
    "ax3 = axes[2]\n",
    "\n",
    "# Interpolate selected FDEM dataset\n",
    "if fdem_plot not in df.columns:\n",
    "    fdem_plot = 'HCP1.0'\n",
    "    warnings.warn(f'Column {fdem_col} not found in dataset. Defaulting to {fdem_plot}.')\n",
    "data_grid = interpolate(df['x'], df['y'], df[fdem_plot], cell_size=0.25)\n",
    "extent = data_grid['extent']\n",
    "\n",
    "# Set units and colormap (cmap) for either IP or ECa data\n",
    "if 'inph' in fdem_plot:\n",
    "        unit = 'IP [ppt]'\n",
    "        cmap = 'gray_r'\n",
    "else:\n",
    "        unit = 'ECa [mS/m]'\n",
    "        cmap = 'viridis_r'\n",
    "\n",
    "# plot interpolated data in left plot (ax1)\n",
    "im = ax1.imshow(data_grid['grid'], \n",
    "                origin='lower', \n",
    "                extent=(extent['x_min'],\n",
    "                        extent['x_max'],\n",
    "                        extent['y_min'],\n",
    "                        extent['y_max']),\n",
    "                cmap = 'viridis_r'\n",
    "                )\n",
    "\n",
    "# plot sample locations and label each point with the sample ID on ax1\n",
    "ax1.plot(ds['x'], ds['y'], 'ko', markersize=15, label='Samples')\n",
    "for index, row in ds.iterrows():\n",
    "    ax1.text(row['x'], row['y'], str(int(row['ID'])), \n",
    "            fontsize=10, ha='center', va='center', color='white')\n",
    "ax1.set_xlabel('x [m]')\n",
    "ax1.set_ylabel('y [m]')\n",
    "ax1.legend()\n",
    "\n",
    "n_int = 100 # number of depth intervals to evaluate the sensitivity\n",
    "\n",
    "# plot coil pair sensitivities in the middle plot (ax2)\n",
    "depths = np.linspace(.0, 2.0, 100)\n",
    "for col in dt.columns:\n",
    "    if col not in ['x','y','z','t','pos'] and 'inph' not in col:\n",
    "        # calculate coil pair sensitivities\n",
    "        rsens_QP, csens_QP, rsens_IP, csens_IP = lin_sens(col, maximum_depth, n_int)\n",
    "\n",
    "        # **************************************************************\n",
    "        # YOU CAN CHANGE THE SENSITIVITY TO BE PLOTTED (plot_sens)\n",
    "        # TO rsens_QP if YOU WANT TO PLOT THE RELATIVE SENSITIVITY\n",
    "        # **************************************************************\n",
    "        plot_sens = csens_QP\n",
    "        if 'PRP' in col:\n",
    "            ax2.plot(plot_sens, depths, linestyle = 'dashed', label=col)\n",
    "        else:\n",
    "            ax2.plot(plot_sens, depths, label=col)\n",
    "\n",
    "ax2.invert_yaxis()\n",
    "ax2.xaxis.tick_top()\n",
    "ax2.xaxis.set_label_position('top')\n",
    "if plot_sens is csens_QP:\n",
    "    ax2.set_xlabel('Cumulative Sensitivity [-]')\n",
    "elif plot_sens is rsens_QP:\n",
    "    ax2.set_xlabel('Relative Sensitivity [-]')  \n",
    "else:\n",
    "    raise ValueError('Error in sensitivity selection')\n",
    "ax2.set_ylabel('Depth [m]')\n",
    "ax2.legend()\n",
    "\n",
    "# Plot relationship between sample data and FDEM data in right plot (ax3)\n",
    "scatter = ax3.plot(ds[sample_col], \n",
    "                    ds[fdem_col],\n",
    "                    'ko', \n",
    "                    markersize=15\n",
    "                    )\n",
    "\n",
    "ax3.set_xlabel(f'{sample_col} {sample_col_unit}')\n",
    "if 'inph' in fdem_col:\n",
    "    ax3.set_ylabel(fdem_col + ' [ppt]')\n",
    "    ax3.set_title(f'{fdem_col} IP vs. {sample_col} {sample_col_unit}.')\n",
    "else:\n",
    "    ax3.set_ylabel(fdem_col + ' ECa [mS/m]')\n",
    "    ax3.set_title(f'{fdem_col} ECa vs. {sample_col}.')\n",
    "\n",
    "# Add labels to scatter points showing the sample ID's\n",
    "for index, row in ds.iterrows():\n",
    "    ax3.text(row[sample_col], row[fdem_col], str(int(row['ID'])), \n",
    "            fontsize=10, ha='center', va='center', color='white')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Export the plots as a PDF\n",
    "if save_to_file==True:\n",
    "    with PdfPages(export_path + '/overview.pdf') as pdf:\n",
    "        pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1: Comparing inverted EC logs with downhole EC data\n",
    "# ------------------------------------------------------\n",
    "\n",
    "save_to_file = False # set to True if you want to save the plots to a PDF\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# get inverted EC data columns from 02_ESS-... .ipynb export file\n",
    "EC_cols = [col for col in d_inv_samps.columns if 'EC' in col[:2] \n",
    "           and len(col) < 8 and 'hydra' not in col]\n",
    "\n",
    "# get interface depths\n",
    "for i, col in enumerate(EC_cols):\n",
    "    if i == 0 and col != 'EC_end':\n",
    "        depth_values = [float(col[3:])]\n",
    "    elif col != 'EC_end':\n",
    "        depth_values.append(float(col[3:]))\n",
    "\n",
    "# Add the last depth value\n",
    "depth_values.append(depth_values[-1] + (depth_values[-1] - depth_values[-2]))\n",
    "\n",
    "unique_sample_ids = d_inv_samps['ID'].unique()\n",
    "\n",
    "# number of subplot rows and columns\n",
    "subplot_rows = 3\n",
    "subplot_cols = 5\n",
    "\n",
    "# Get axis limits based on conductivity values in inverted and field datasets\n",
    "global_x_min = min(d_inv_samps[EC_cols].min().min(), d_ec_logs['EC_msm'].min())\n",
    "global_x_max = max(d_inv_samps[EC_cols].max().max(), d_ec_logs['EC_msm'].max())\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(subplot_rows, subplot_cols, figsize=(15, 9), sharey=True, sharex=True)\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "# Loop through each sample ID and plot the EC values\n",
    "for i, sample_id in enumerate(unique_sample_ids):\n",
    "    # Get row index for the current sample ID\n",
    "    row_index = d_inv_samps.loc[d_inv_samps['ID'] == sample_id].index[0]\n",
    "    row_data = d_inv_samps.loc[row_index, EC_cols].values\n",
    "    # Get the current subplot axis\n",
    "    ax = axes[i // subplot_cols, i % subplot_cols]\n",
    "    # Get the EC logs data for the current sample ID\n",
    "    data = d_ec_logs[d_ec_logs['ID'] == sample_id]\n",
    "    # append additional depth value that is 0.10 m deeper than last depth value\n",
    "    new_row = pd.DataFrame([{'EC_msm': data['EC_msm'].iloc[-1], \n",
    "                             'depth_m': data['depth_m'].iloc[-1] + 0.10}])\n",
    "    data = pd.concat([data, new_row], ignore_index=True)\n",
    "\n",
    "    if sample_id == 1:\n",
    "        ax.step(row_data, depth_values, label=f'Modelled EC')\n",
    "        ax.step(data['EC_msm'], data['depth_m'], label=f'EC logs')\n",
    "    else:\n",
    "        ax.step(row_data, depth_values)\n",
    "        ax.step(data['EC_msm'], data['depth_m'])\n",
    "    if i // subplot_cols == subplot_rows - 1:\n",
    "        ax.set_xlabel('EC [mS/m]')\n",
    "    if i % subplot_cols == 0:\n",
    "        ax.set_ylabel('Depth [m]')\n",
    "    ax.set_title(f'Sample {int(sample_id)}')\n",
    "    ax.set_xlim(global_x_min, global_x_max)\n",
    "    ax.invert_yaxis()\n",
    "    ax.grid(True)\n",
    "fig.legend(bbox_to_anchor=(0.3, 0.95), ncol=2)\n",
    "fig.suptitle('Modelled EC profiles at sampling locations', fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Export the plots as a PDF\n",
    "if save_to_file==True:\n",
    "    with PdfPages(export_path + '/modECprofiles_sampleloc.pdf') as pdf:\n",
    "        pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Pedophysical modelling - predicting bulk EC\n",
    "\n",
    "Pedophysical models are models that aim to describe the relationship between the observed geophysical properties, and the soil properties and states of interest. Generally, a geophysical property of a soil (i.e., a soil being a medium that integrates multiple phases) is influenced by the soil constituents (i.e., the relative volumetric proportion of the soil components and their physical properties), and the soil structure (i.e., the spatial distribution of the soil constituents and their interconnection). \n",
    "\n",
    "### Archie's Law\n",
    "In class, we have briefly discussed Archie's law, which, when modified for soils, primarily accounts for the conductivity of the pore fluid ($\\sigma_{wat}$), and considers the degree of saturation $S_{wat}$ alongside a saturation exponent $n$, which relates to the pore structure of the medium. The degree of saturation equals the ratio of the volumetric water content ($\\theta$) to the porosity ($\\phi$) of the medium:\n",
    "\n",
    "$$\n",
    "S_{wat} = \\frac{\\theta}{\\phi}\n",
    "$$\n",
    "\n",
    "\n",
    "The combined Archie’s law is:\n",
    "\n",
    "$$\n",
    "\\sigma = \\phi^{m} \\times S_{wat}^{n} \\times \\sigma_{wat}.\n",
    "$$\n",
    "\n",
    "This allows estimating the conductivity $\\sigma$ for partially saturated media whereby $m$ (fixed at 1.5) and $n$ (fixed at 2) allow including the influence of the medium’s structure.\n",
    "____\n",
    "<br>\n",
    "\n",
    "### Waxman and Smits equation \n",
    "One major shortcoming of Archie’s law was originally developed for sandy rocks, which presents significant shortcomings when applied to other media such as soils. Most importantly, surface conduction (i.e., the conduction along the surface of dry particles) is not taken into account. In all soils, the influence of surface conductance is non-negligible, and increases with clay content. Other models account for this, such as the adjustment proposed by [Waxman & Smits (1968)](https://users.ugent.be/~pjdsmedt/ESS2023/WS_1986.pdf), who quantify the surface conductivity ($\\sigma_{surface}$) by defining a parameter $Q_V$ based on the cation exchange capacity ($CEC$):\n",
    "\n",
    "\n",
    "$$\n",
    "Q_v = pd \\frac{1-\\phi}{\\phi} \\times CEC ,\n",
    "$$\n",
    "\n",
    "with $pd$ the particle density. This results in the following relationship with bulk electrical conductivity:\n",
    "\n",
    "$$\n",
    "\\sigma = \\frac{1}{\\phi^{-2}} \\times (\\sigma_{water} + B \\times Q_v),\n",
    "$$\n",
    "\n",
    "resulting in the surface conductivity:\n",
    "\n",
    "$$\n",
    "\\sigma_{surface} = (B \\times Q_v \\times S_{wat} \\times \\phi^{m}).\n",
    "$$\n",
    "\n",
    "Hereby, $B$ represents an equivalent counterion (sodium clay-exchange cations) conductivity that is determined empirically ([Revil et al. 1998](https://agupubs.onlinelibrary.wiley.com/doi/10.1029/98JB02125)).\n",
    "___\n",
    "<br>\n",
    "\n",
    "### Linde et al. 2006\n",
    "More recently, a push towards more practical integrations of the surface conductivity\n",
    "The equation from [Linde et al. 2006](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2006WR005131) mentioned in [Romero-Ruiz et al. 2018](http://doi.wiley.com/10.1029/2018RG000611), integrates the surface conductivity as follows:\n",
    "\n",
    "$$\n",
    "\\sigma = \\phi^{m} \\times S_{wat}^{n} \\times \\sigma_{water} + (1 - \\phi^{m}) \\times \\sigma_{surface} .\n",
    "$$\n",
    "\n",
    "Hereby, the surface conductivity is derived from soil texture, through empirical observations, by attributing more weight to a set conductivity of solid particles as the particle size decreases.\n",
    "\n",
    "### Fu et al. 2021\n",
    "An alternative formulation of this approach was presented by [Fu et al., 2021](https://www.sciencedirect.com/science/article/pii/S0022169421002079):\n",
    "\n",
    "$$\n",
    "\\sigma = \\sigma_{water} \\times \\theta^{w} + \\theta \\times \\phi \\times \\sigma_{surface} + (1-\\phi) \\times \\sigma_{solid},\n",
    "$$\n",
    "\n",
    "with a constant $w$ that has an average values of 2, respectively. The conductivity of the solid phase, which is negligible, is included as $\\sigma_{solid}$. \n",
    "Based on empirical data, the surface conductivity $\\sigma_{surface}$ is then simplified as:\n",
    "\n",
    "$$\n",
    "\\sigma_{surface} = 0.654\\frac{clay}{sand + silt} + 0.0183 ,\n",
    "$$\n",
    "\n",
    "following [Doussan & Ruy (2009)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2008WR007309). For the function presented by [Fu et al. 2021](https://www.sciencedirect.com/science/article/pii/S0022169421002079), extensive calibration was performed based on soil samples with clay content ranging from 0 to 33%.\n",
    "\n",
    "An overview of these and other pedophysical models can be found in [Mendoza et al. (2024)](https://arxiv.org/pdf/2403.07473.pdf).\n",
    "\n",
    "The equations incorporating surface conductivity are provided in **_code cell 2.0_** as `waxsmith`, `linde`, and `fu`, respectively. \n",
    "You can test these for a range of varying soil properties in **_code cell 2.1_**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.0: Pedophysical modelling - Linde et al. 2006\n",
    "# -----------------------------------------------\n",
    "help(waxsmits)\n",
    "help(linde)\n",
    "help(fu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic evaluation of pedophysical models\n",
    "\n",
    "In the following two code cells you can evaluate the pedophysical models presented above dynamically, by modifying input parameter values with sliders. If needed, you can adjust the starting values and ranges for the sliders. Keep in mind that setting all slides to a maximum value can lead to unrealistic scenarios (e.g., unrealistically high bulk densities for clayey soils)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1: Evaluate the Waxman-Smits model with Revil et al. 1998 modification\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "In this cell, code is provided to evaluate the Wasmits model with the \n",
    "Revil et al. 1998 modification. You can modify the starting values and ranges\n",
    "for the relevant parameters (CEC, VWC, BD, ECW) to evaluate the model's outcome\n",
    "in different scenarios.\n",
    "\"\"\"\n",
    "\n",
    "# Set starting values and ranges for sliders\n",
    "min_CEC = 1; max_CEC = 50\n",
    "min_bd = 1.0; max_bd = 2.1\n",
    "min_vwc = 10; max_vwc = 50\n",
    "min_ecw = 10; max_ecw = 100   \n",
    "\n",
    "# ******************************************************************** #\n",
    "# Create empty arrays to populate with values\n",
    "iterations = 100\n",
    "CEC_i = np.linspace(min_CEC, max_CEC, iterations)\n",
    "vwc_i = np.linspace(min_vwc, max_vwc, iterations)\n",
    "b_dens_i = np.linspace(min_bd, max_bd, iterations)\n",
    "\n",
    "# Update plot function\n",
    "# we used updated plot function \n",
    "\n",
    "# Define sliders\n",
    "CEC_slider = FloatSlider(value=min_CEC, min=min_CEC, max=max_CEC, \n",
    "                         step=0.1, description='CEC [mmol/g]')\n",
    "VWC_slider = FloatSlider(value=min_vwc, min=min_vwc, max=max_vwc, \n",
    "                         step=0.1, description='VWC [%]')\n",
    "BD_slider = FloatSlider(value=min_bd, min=min_bd, max=max_bd, \n",
    "                        step=0.01, description='BD [g/cm^3]')\n",
    "ECW_slider = FloatSlider(value=min_ecw, min=min_ecw, max=max_ecw, \n",
    "                         step=0.1, description='EC_w [mS/m]')\n",
    "\n",
    "# Use interact to create interactive widgets\n",
    "interact(\n",
    "    lambda CEC, VWC, ECW, BD: update_plot2(CEC, VWC, BD, ECW, vwc_i, b_dens_i, CEC_i),\n",
    "    CEC=CEC_slider,\n",
    "    VWC=VWC_slider,\n",
    "    BD=BD_slider,\n",
    "    ECW=ECW_slider\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1: Evaluate the Linde et al 2016 and Fu et al. 2021 models\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\"\"\" \n",
    "In this cell, code is provided to evaluate the Linde et al. 2006 and Fu et al.\n",
    "2021 models. You can modify the starting values and ranges for the relevant\n",
    "parameters (clay, VWC, ECW, BD) to evaluate the model's outcome in different\n",
    "scenarios.\n",
    "\"\"\"\n",
    "\n",
    "# Set starting values and ranges for sliders\n",
    "min_clay = 5; max_clay = 40\n",
    "min_vwc = 10; max_vwc = 50\n",
    "min_ecw = 10; max_ecw = 100\n",
    "min_bd = 1.0; max_bd = 2.1\n",
    "\n",
    "# getting clay and bd range n iterations\n",
    "iterations = 100\n",
    "clay_i = np.linspace(min_clay, max_clay, iterations)\n",
    "b_dens_i = np.linspace(min_bd, max_bd, iterations)\n",
    "vwc_i = np.linspace(min_vwc, max_vwc, iterations)\n",
    "\n",
    "# Define sliders for each parameter and assure that sliders are positioned below the plots\n",
    "clay_slider = FloatSlider(value=min_clay, min=min_clay, max=max_clay, step=0.1, description='Clay content [%]', style={'description_width': 'initial'}) \n",
    "vwc_slider = FloatSlider(value=min_vwc, min=min_vwc, max=max_vwc, step=0.1, description='VWC [%]', style={'description_width': 'initial'}) \n",
    "ecw_slider = FloatSlider(value=min_ecw, min=min_ecw, max=max_ecw, step=0.1, description='EC_w [mS/m]', style={'description_width': 'initial'}) \n",
    "bd_slider = FloatSlider(value=min_bd, min=min_bd, max=max_bd, step=0.01, description='BD [g/cm^3]', style={'description_width': 'initial'}) \n",
    "\n",
    "# Use interact to create interactive widgets\n",
    "interact(\n",
    "    lambda CLAY, VWC, ECW, BD: update_plot(CLAY, VWC, ECW, BD, vwc_i, b_dens_i, clay_i),\n",
    "    CLAY=clay_slider,\n",
    "    VWC=vwc_slider,\n",
    "    ECW=ecw_slider,\n",
    "    BD=bd_slider,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating sampling data\n",
    "\n",
    "In **_code cell 2.2_** you can evaluate the performance of these models by comparing the modelled soil EC values based on the analytical data, and comparing these to point EC data collected with the HydraProbe during fieldwork, and any of the FDEM datasets. Just specify the FDEM dataset to evaluate. You can choose the fix the conductivity on the pore solution, to evaluate what impact this has on the prediction, by setting the `fix_ec_water` variable to `True`. This will use the average value of the pore solution conductivity as measured with the HydraProbe during fieldwork.  \n",
    "\n",
    "By running **_code_cell 2.3_** you can add the EC values modelled through the Linde et al. (2006) equation to the samples dataframe (dataframe `ds`), and exporting it as a csv file to your Google drive.\n",
    "\n",
    "_**REMINDER** to check the column names for a specific pandas dataframe, just visualise the '.columns' attribute, for instance as `ds.columns` for the `ds` dataframe that holds the sample data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2: Evaluating pedophysical relationships at sampling locations\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "save_to_file = False # set to True if you want to save the plots to a PDF\n",
    "\n",
    "# Set the column names for EC datasets (plotted on x-axes) you want to compare \n",
    "# with the EC values predicted by the pedophysical models (plotted on y-axes)\n",
    "\n",
    "EC_col = 'EC_bulk_hydra' # Default = hydraprobe bulk EC values \n",
    "fdem_col = 'HCP0.5' # Default = 0.5 m HCP FDEM dataset. You can change this to columns of the inverted FDEM data at sampling locations\n",
    "\n",
    "# Set conductivity of the pore solution as fixed (ec_water_avg variable)'\n",
    "\n",
    "fix_ec_water = False # if False, for each sample location the EC_water value \n",
    "                     # measured by the HydraProbe is used\n",
    "ec_water_avg = d_inv_samps['EC_water_hydra'].median() # fixed EC_water [mS/m]\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# Initialize empty lists to store the predicted bulk_ec values\n",
    "predicted_bulk_ec_linde = []\n",
    "predicted_bulk_ec_fu = []\n",
    "predicted_bulk_ec_ws = []\n",
    "\n",
    "# Loop through the rows of the DataFrame\n",
    "for index, row in d_inv_samps.iterrows():\n",
    "    # Get the input values from the DataFrame\n",
    "    vmc = row['vmc_hydra']\n",
    "    bd = row['bd_10cm']\n",
    "    sand = row['sand_10cm']\n",
    "    CEC = row['CEC_10cm']\n",
    "    silt = row['silt_10cm']\n",
    "    clay = row['clay_10cm']\n",
    "    if fix_ec_water:\n",
    "        water_ec = ec_water_avg\n",
    "    else:\n",
    "        water_ec = row['EC_water_hydra']\n",
    "    \n",
    "    # Calculate the bulk electrical conductivity based on linde and fu models\n",
    "    bulk_ec_linde = linde(vmc, bd, sand, clay, water_ec)\n",
    "    bulk_ec_fu = fu(vmc, bd, water_ec, clay)\n",
    "    # waxsmits(vwc, bd, water_ec, CEC, pdn=2.65, m=1.5, n=2, a = 0.4):\n",
    "    bulk_ec_ws = waxsmits(vmc, bd, water_ec, CEC)\n",
    "\n",
    "    # Store the predicted bulk_ec values\n",
    "    predicted_bulk_ec_linde.append(bulk_ec_linde)\n",
    "    predicted_bulk_ec_fu.append(bulk_ec_fu)\n",
    "    predicted_bulk_ec_ws.append(bulk_ec_ws)\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=[12, 5])\n",
    "\n",
    "# Plot bulk_ec versus hydra_ec [mS/m]\n",
    "axes[0].scatter(d_inv_samps[EC_col], predicted_bulk_ec_linde, alpha=0.7)\n",
    "axes[0].scatter(d_inv_samps[EC_col], predicted_bulk_ec_fu, alpha=0.7)\n",
    "axes[0].scatter(d_inv_samps[EC_col], predicted_bulk_ec_ws, alpha=0.7, color='gray')\n",
    "\n",
    "# # UNCOMMENT TO ADD A TRENDLINE TO EACH PLOT\n",
    "# # add trendline to each plot\n",
    "# slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(ds['hydra_ec [mS/m]'], predicted_bulk_ec_linde)\n",
    "# line = slope * ds['hydra_ec [mS/m]'] + intercept\n",
    "# axes[0].plot(ds['hydra_ec [mS/m]'], line, label='Linde et al. 2006')\n",
    "# slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(ds['hydra_ec [mS/m]'], predicted_bulk_ec_fu)\n",
    "# line = slope * ds['hydra_ec [mS/m]'] + intercept\n",
    "# axes[0].plot(ds['hydra_ec [mS/m]'], line, label='Fu et al. 2021')\n",
    "# slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(ds['hydra_ec [mS/m]'], predicted_bulk_ec_ws)\n",
    "# line = slope * ds['hydra_ec [mS/m]'] + intercept\n",
    "# axes[0].plot(ds['hydra_ec [mS/m]'], line, label='Waxman-Smits/Revill')\n",
    "# axes[0].legend()\n",
    "\n",
    "axes[0].set_xlabel(\"hydra_ec [mS/m]\")\n",
    "axes[0].set_ylabel(\"Predicted EC [mS/m]\")\n",
    "#axes[0].set_title(\"Predicted EC vs hydra_ec\")\n",
    "\n",
    "# Plot bulk_ec versus selected FDEM column LIN ECa [mS/m]\n",
    "axes[1].scatter(ds[fdem_col], predicted_bulk_ec_linde, alpha=0.9, \n",
    "                label = \"Linde et al. 2006\")\n",
    "axes[1].scatter(ds[fdem_col], predicted_bulk_ec_fu, alpha=0.7, \n",
    "                label = \"Fu et al. 2021\")\n",
    "axes[1].scatter(ds[fdem_col], predicted_bulk_ec_ws, alpha=0.5, color='gray', \n",
    "                label = \"Waxman-Smits/Revill\")\n",
    "axes[1].legend()\n",
    "axes[1].set_xlabel(fdem_col + \" ECa [mS/m]\")\n",
    "axes[1].set_ylabel(\"Predicted EC [mS/m]\")\n",
    "#axes[1].set_title(f\"Predicted EC vs {fdem_col}\")\n",
    "\n",
    "fig.suptitle('Comparison of observed EC values vs predicted bulk EC'\n",
    "              ' from pedophysical models', fontsize=14)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if save_to_file==True:\n",
    "    \n",
    "    with PdfPages(export_path + '/obsECvspredEC.pdf') as pdf:\n",
    "        pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3: Store predicted EC values in the `d_inv_samps` dataframe and save .csv\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "Here you can export the predicted bulk EC values from the pedophysical models\n",
    "to a .csv file. Per default, this will be a file that combines your inversion\n",
    "results at the sampling locations, the FDEM ECa data, the sample information,\n",
    "and the predicted bulk EC values from the Linde, Fu and Waxman-Smits models.\n",
    "\"\"\"\n",
    "\n",
    "# Define a filename for exporting\n",
    "filename = 'samples_and_predictions.csv'\n",
    "\n",
    "# ******************************************************************** #\n",
    "# Create column for predicted EC values (in mS/m) in the d_inv_samps dataframe and export as csv\n",
    "d_inv_samps['Linde EC_predict [mS/m]'] = predicted_bulk_ec_linde\n",
    "d_inv_samps['Fu EC_predict [mS/m]'] = predicted_bulk_ec_fu\n",
    "d_inv_samps['Waxman-Smits EC_predict [mS/m]'] = predicted_bulk_ec_ws\n",
    "d_inv_samps.to_csv(export_path + '/' + filename, index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exploring relationships between target properties and observed geophysical variations.\n",
    "\n",
    "To evaluate to which extent the collected FDEM data can help predict the spatial variability of your group's target property, it is important to understand the relationship between the sensor output, and the targeted properties. This can be done at the sampling locations by further exploring the `d_inv_samps` dataframe that contains the FDEM data and analytical data at these locations, as well as your inversion results ath these locations. For instance, you can compare the FDEM ECa data to a given property at a given depth, and make the same comparision with the Hydraprobe EC data.\n",
    "\n",
    "## Stochastic modelling\n",
    "A stochastic empirical approach can be used as a basis for predictions. As an example, a regression analysis is included in **_code cell 3.1_**. \n",
    "The obtained linear function is incorporated in the 'lin_fit' object. You can obtain the function by simply printing the objects as: `print(lin_fit)`, as is done in **_code cell 3.1_**. \n",
    "\n",
    "To extract the slope and intercept from the linear model, you can directly access these as:\n",
    "\n",
    "`slope = lin_fit[1]` <br />\n",
    "`intercept = lin_fit[0]`,\n",
    "\n",
    "Suited models, that avoid overfitting and provide satisfactory prediction errors, can then be used to predict the target variable based on the obtained model. \n",
    "Alongside the regression analysis, a cross-validation is performed using the 'leave-one-out' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0: evaluating relationships between target properties and conductivity data\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "save_to_file=False # set to True if you want to save the plots to a PDF \n",
    "\n",
    "target = 'pH_H2O_10cm' # ds columnname for column with target property \n",
    "target_unit = '[-]' # unit of the target property\n",
    "\n",
    "fdem_col = 'EC_end' # ds columnname for column with FDEM dataset (inverted or non-inverted)\n",
    "hydra_col = 'EC_bulk_hydra' # ds columnname for hydraprobe data column\n",
    "\n",
    "# create a copy of the sample dataset to plot and, if needed, filter out\n",
    "# specific points (outliers/validation data/...)\n",
    "ds_in = d_inv_samps.copy()\n",
    "\n",
    "# # uncomment these lines to remove the validation or other points \n",
    "# exclude_points = [11, 12, 13, 14, 15] # replace with point ID's you want to remove\n",
    "# ds_in = ds_in[~ds_in['ID'].isin(exclude_points)]\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Plotting\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,7))\n",
    "# Define different axes for plotting\n",
    "\n",
    "# FDEM compare plot\n",
    "ax[0].scatter(ds_in[fdem_col],ds_in[target], s=200)\n",
    "ax[0].set_xlabel(f'{fdem_col} (mS/m) data')\n",
    "ax[0].set_ylabel(f'{target} {target_unit}')\n",
    "# add sample ID labels to the plot\n",
    "for index, row in ds_in.iterrows():\n",
    "    ax[0].text(row[fdem_col], row[target], str(int(row['ID'])), \n",
    "    fontsize=10, ha='center', va='center', color='white')\n",
    "ax[0].grid()\n",
    "ax[0].set_title(f'{fdem_col} vs. {target}')\n",
    "\n",
    "# HyrdaGo compare plot\n",
    "ax[1].scatter(ds_in[hydra_col],ds_in[target], s=200)\n",
    "ax[1].set_xlabel(f'{hydra_col} (mS/m) data')\n",
    "ax[1].set_ylabel(f'{target} {target_unit}')\n",
    "\n",
    "#   add sample ID labels to the plot\n",
    "for index, row in ds_in.iterrows():\n",
    "    ax[1].text(row[hydra_col], row[target], str(int(row['ID'])), \n",
    "    fontsize=10, ha='center', va='center', color='white')\n",
    "ax[1].grid()\n",
    "ax[1].set_title(f'{hydra_col} vs. {target}')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if save_to_file==True:\n",
    "    with PdfPages(export_path + '/target_vs_EC.pdf') as pdf:\n",
    "        pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1: Perform a linear regression with numpy and evaluate the model\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "save_to_file=False # set to True if you want to save the plots to a PDF\n",
    "\n",
    "target = 'pH_H2O_10cm' # d_inv_samps column name for target property\n",
    "fdem_col = 'HCP1.0' # d_inv_samps column name for FDEM dataset (inverted or ECa)\n",
    "\n",
    "# ******************************************************************** #\n",
    "\n",
    "# create columnname with name based on target variable\n",
    "col_name = f'predicted_{target}'\n",
    "\n",
    "# Ensure the dataset is clean and well-prepared\n",
    "ds_in.dropna(inplace=True)\n",
    "ds_in[fdem_col] = pd.to_numeric(ds_in[fdem_col], errors='coerce')\n",
    "ds_in[hydra_col] = pd.to_numeric(ds_in[hydra_col], errors='coerce')\n",
    "ds_in[target] = pd.to_numeric(ds_in[target], errors='coerce')\n",
    "\n",
    "# Extract features and target variables\n",
    "X = ds_in[fdem_col].values.reshape(-1, 1)\n",
    "y = ds_in[target].values\n",
    "\n",
    "# Sort the dataset based on the feature variable\n",
    "X_sorted = np.sort(X, axis=0)\n",
    "\n",
    "# numpy\n",
    "lin_fit = np.poly1d(np.polyfit(ds_in[fdem_col].values, \n",
    "                               ds_in[target].values, 1))\n",
    "ds_in[col_name] = lin_fit(ds_in[fdem_col].values)\n",
    "\n",
    "# Calculate R-squared for model using 'leave-one-out' cross-validation\n",
    "# implemented via scikit-learn (sklearn)\n",
    "\n",
    "# Calculate R-squared for linear model\n",
    "r2_linear_numpy = r2_score(y, lin_fit(X))\n",
    "\n",
    "# Cross-validation (leave-one-out)\n",
    "# --------------------------------\n",
    "# iterate over the dataset and calculate the MSE for each iteration using\n",
    "# the leave-one-out via Scikit-learn\n",
    "loo = LeaveOneOut()\n",
    "mse_scores_numpy = []\n",
    "for train_index, test_index in loo.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    lin_fit = np.poly1d(np.polyfit(X_train[:,0], y_train, 1))\n",
    "    y_pred_numpy_eval = lin_fit(X_test[:, 0])\n",
    "\n",
    "    # calculate the mean squared error for each iteration\n",
    "    mse_scores_numpy.append(mean_squared_error(y_test, y_pred_numpy_eval))\n",
    "\n",
    "# calculate the average mean squared error\n",
    "average_mse_numpy = np.mean(mse_scores_numpy)\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(1,2, figsize=(15,7))\n",
    "\n",
    "# plot the linear regression model against input vs target data\n",
    "ax[0].scatter(ds_in[fdem_col], ds_in[target], \n",
    "              color='black', label='Observations')\n",
    "ax[0].plot(ds_in[fdem_col], ds_in[col_name], \n",
    "           label=f'Linear regression, R2 = {r2_linear_numpy:.3f}')\n",
    "ax[0].set_xlabel('ECa [mS/m]')\n",
    "ax[0].set_ylabel(f'{target} {target_unit}')\n",
    "ax[0].legend()\n",
    "\n",
    "# plot comparison of observed and predicted target property\n",
    "# add 1:1 line\n",
    "ax[1].plot([ds_in[target].min(), ds_in[target].max()], \n",
    "            [ds_in[target].min(), ds_in[target].max()], \n",
    "            color='red', alpha=0.3, linestyle='--')\n",
    "ax[1].scatter(ds_in[target], ds_in[col_name], s=200)\n",
    "# add sample ID labels\n",
    "for index, row in ds_in.iterrows():\n",
    "    ax[1].text(row[target], row[col_name], str(int(row['ID'])), \n",
    "            fontsize=10, ha='center', va='center', color='white')\n",
    "\n",
    "ax[1].set_xlabel(f'Observed {target} {target_unit}')\n",
    "ax[1].set_ylabel(f'Predicted {target} {target_unit}')\n",
    "ax[1].set_title('average MSE: {:.3f}'.format(average_mse_numpy))\n",
    "# Set ax[1] limits\n",
    "combined_min = min(ds_in[target].min(), ds_in[col_name].min())\n",
    "combined_max = max(ds_in[target].max(), ds_in[col_name].max())\n",
    "ax[1].set_xlim([combined_min-1, combined_max-1])\n",
    "ax[1].set_ylim([combined_min-1, combined_max+1])\n",
    "ax[1].grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "if save_to_file==True:\n",
    "    with PdfPages(export_path + '/linear_regression.pdf') as pdf:\n",
    "        pdf.savefig(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform prediction for entire survey area\n",
    "\n",
    "In **_code cell 3.2_** you can apply a suited model to the entire FDEM dataset. You can specify the target and the data feature (FDEM dataset or inverted EC layer) to preform the prediction with. Make sure that you use the suited model (i.e., use the same input data as you trained your model on). \n",
    "In the cell, you can interpolate and export the predicted property map to a georeferenced raster, as well as to a .csv file.\n",
    "\n",
    "You can then import these data into your GIS and compare it to available information on your target property. \n",
    "\n",
    "_Sidenote: to change the plot colormaps, you find reference info on the [Matplotlib documentation site](https://matplotlib.org/stable/users/explain/colors/colormaps.html)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2: Predict a target property based on the regression model from code cell 3.1\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# If needed, you can replace d_inv with another dataframe \n",
    "# The d_inv dataframe contains the full inverted (see code cell 0.1)\n",
    "dpred_in = d_inv.copy() \n",
    "\n",
    "# Set the dataframe column name that holds the EC values.For the inverted\n",
    "# EC values, the column names depend on the boundaries you have set.\n",
    "fdem_col = 'EC_0.60'\n",
    "\n",
    "# column name for the EC values you want to use for the prediction\n",
    "# SAME AS SPECIFIED FOR THE MODEL YOU CREATED IN 3.1\n",
    "target = target \n",
    "col_name = f'predicted_{target}' # column name for the predicted target property\n",
    "\n",
    "# Interpolate the predicted target property values to a grid and export\n",
    "cell_size = 0.25\n",
    "export_raster = True # if true the interpolated data will be exported as a geoTIFF\n",
    "export_csv = True # if true the interpolated data will be exported as a CSV\n",
    "\n",
    "# ******************************************************************** #\n",
    "# Run prediction based on the linear model from cell 3.1\n",
    "dpred_in[col_name] = lin_fit(dpred_in[fdem_col].values)\n",
    "\n",
    "# Interpolate results and plot the outcome\n",
    "cell_size = 0.25\n",
    "\n",
    "prediction_grid = interpolate(dpred_in['x'], dpred_in['y'], dpred_in[col_name], cell_size=cell_size, blank=blank)\n",
    "grid_name = f'{col_name}_grid'\n",
    "# Specify the grid extent for plotting with correct x-y coordinates\n",
    "extent = prediction_grid['extent']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(7,7))\n",
    "im = ax.imshow(prediction_grid['grid'], \n",
    "                origin='lower', \n",
    "                extent=(extent['x_min'],\n",
    "                        extent['x_max'],\n",
    "                        extent['y_min'],\n",
    "                        extent['y_max']),\n",
    "                cmap = 'copper_r'\n",
    "                )\n",
    "# Set limits to the plotting range based on data percentiles by \n",
    "# uncommenting the 4 lines below: \n",
    "\n",
    "# pmin = 2  # lower percentile\n",
    "# pmax = 98  # upper percentile \n",
    "# im.set_clim(np.percentile(data_grid['grid'].flatten()[~np.isnan(data_grid['grid'].flatten())], pmin),\n",
    "#         np.percentile(data_grid['grid'].flatten()[~np.isnan(data_grid['grid'].flatten())], pmax))\n",
    "\n",
    "ax.set_title(f\"{col_name} {target_unit}\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "if export_raster:\n",
    "    # Export the interpolated data as a GeoTIFF\n",
    "    export_grid(prediction_grid, filename=grid_name, export_path=export_path)\n",
    "\n",
    "if export_csv:\n",
    "   # generate dataframe with only predicted values (col_name) and columns 'x', 'y', and the fdem_col\n",
    "    dpred_out = dpred_in[['x', 'y', fdem_col, col_name]]\n",
    "    dpred_out.to_csv(export_path + f'/{col_name}_grid.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
